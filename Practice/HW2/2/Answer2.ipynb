{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUgLFmyB_U0f"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJhWcrOs_WiD"
      },
      "source": [
        "# 1.3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "popPGLyhjlH5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "GZTneAhLj4K8"
      },
      "outputs": [],
      "source": [
        "class PreProcess:\n",
        "    def __init__(self):\n",
        "        self.dataSet = pd.read_csv(\"./teleCust1000t.csv\")\n",
        "\n",
        "    def plot_heatmap(self):\n",
        "        corr_matrix = self.dataSet.corr()\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "        plt.title('Correlation Matrix Heatmap')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_histogram(self):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(self.dataSet['ed'], kde=True, bins=30, color='blue')\n",
        "        plt.title(f\"Histogram of ed\")\n",
        "        plt.xlabel('ed')\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(self.dataSet['tenure'], kde=True, bins=30, color='blue')\n",
        "        plt.title(f\"Histogram of tenure\")\n",
        "        plt.xlabel('tenure')\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def normalize(self):\n",
        "        X = self.dataSet.drop(columns=['custcat']).values\n",
        "        y = self.dataSet['custcat'].values\n",
        "        # Adjust labels to be zero-indexed\n",
        "        self.y = y - y.min()\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        self.X = scaler.fit_transform(X)\n",
        "\n",
        "    def split_data(self):\n",
        "        X_train_full, X_test, y_train_full, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=93)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=93)\n",
        "\n",
        "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "        X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "        y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "        y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "        # Create DataLoaders for batching\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "        # Number of input features and classes\n",
        "        input_size = X_train.shape[1]\n",
        "        num_classes = len(torch.unique(y_train))\n",
        "\n",
        "        return {\"input size\": input_size, \"classes\": num_classes, \"train\": train_loader, \"test\": test_loader, \"val\": val_loader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "__ENM8yWSo_Y"
      },
      "outputs": [],
      "source": [
        "preprocess = PreProcess()\n",
        "preprocess.plot_heatmap()\n",
        "preprocess.plot_histogram()\n",
        "preprocess.normalize()\n",
        "data = preprocess.split_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "YphBCe2_Ehzu",
        "outputId": "5d1c30e4-561c-47e5-e44e-1d048971b26f"
      },
      "outputs": [],
      "source": [
        "input_size = data[\"input size\"]\n",
        "num_classes = data[\"classes\"]\n",
        "train_loader = data[\"train\"]\n",
        "val_loader = data[\"val\"]\n",
        "test_loader = data[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IWU7Nk2L3i2x",
        "outputId": "7010d979-a8c0-45fe-f55a-d0c0e283cfe3"
      },
      "outputs": [],
      "source": [
        "# Model 1: Single hidden layer with optional Batch Normalization and Dropout\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, use_batch_norm=False, dropout_prob=0.0):\n",
        "        super(Model1, self).__init__()\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.hidden = nn.Linear(input_size, 80)\n",
        "        self.batch_norm_hidden = nn.BatchNorm1d(80) if use_batch_norm else None\n",
        "        self.dropout = nn.Dropout(dropout_prob) if dropout_prob > 0 else None\n",
        "        self.output = nn.Linear(80, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm_hidden(x)\n",
        "        x = torch.relu(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# Model 2: Two hidden layers with optional Batch Normalization and Dropout\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, use_batch_norm=False, dropout_prob=0.0):\n",
        "        super(Model2, self).__init__()\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.hidden1 = nn.Linear(input_size, 100)\n",
        "        self.batch_norm_hidden1 = nn.BatchNorm1d(100) if use_batch_norm else None\n",
        "        self.dropout1 = nn.Dropout(dropout_prob) if dropout_prob > 0 else None\n",
        "        self.hidden2 = nn.Linear(100, 50)\n",
        "        self.batch_norm_hidden2 = nn.BatchNorm1d(50) if use_batch_norm else None\n",
        "        self.dropout2 = nn.Dropout(dropout_prob) if dropout_prob > 0 else None\n",
        "        self.output = nn.Linear(50, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden1(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm_hidden1(x)\n",
        "        x = torch.relu(x)\n",
        "        if self.dropout1:\n",
        "            x = self.dropout1(x)\n",
        "\n",
        "        x = self.hidden2(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm_hidden2(x)\n",
        "        x = torch.relu(x)\n",
        "        if self.dropout2:\n",
        "            x = self.dropout2(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GZFN737Bk5c",
        "outputId": "68c9a654-e75f-4395-86d8-50be8b37c01d"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    \"\"\"\n",
        "    A flexible wrapper for neural network training and evaluation.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_layer (int): Determines the type of model to use (1 or 2).\n",
        "        optimizer (str): Specifies the optimizer ('SGD', 'Adam', or 'Adadelta').\n",
        "        input_size (int): Number of input features.\n",
        "        num_classes (int): Number of output classes.\n",
        "        use_batch_norm (bool): Whether to use batch normalization.\n",
        "        dropout_prob (float): Dropout probability (default: 0.0).\n",
        "        weight_decay (float): Weight decay for regularization (default: 0.0).\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_layer, optimizer, input_size, num_classes, \n",
        "                 use_batch_norm=False, dropout_prob=0.0, weight_decay=0.0):\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        if hidden_layer == 1:\n",
        "            self.model = Model1(input_size, num_classes, use_batch_norm, dropout_prob)\n",
        "        elif hidden_layer == 2:\n",
        "            self.model = Model2(input_size, num_classes, use_batch_norm, dropout_prob)\n",
        "        else:\n",
        "            raise ValueError(\"hidden_layer must be 1 or 2\")\n",
        "\n",
        "        if optimizer == \"SGD\":\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "        elif optimizer == \"Adam\":\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001, weight_decay=weight_decay)\n",
        "        elif optimizer == \"Adadelta\":\n",
        "            self.optimizer = optim.Adadelta(self.model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported optimizer. Choose from: 'SGD', 'Adam', 'Adadelta'\")\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(X_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for X_batch, y_batch in val_loader:\n",
        "                    outputs = self.model(X_batch)\n",
        "                    loss = self.criterion(outputs, y_batch)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            val_losses.append(val_loss / len(val_loader))\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "        # Plot training and validation loss\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.title('Loss over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate_model(self, test_loader):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a test dataset.\n",
        "\n",
        "        Args:\n",
        "            test_loader: DataLoader for test data.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                outputs = self.model(X_batch)\n",
        "                test_loss += self.criterion(outputs, y_batch).item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == y_batch).sum().item()\n",
        "\n",
        "        accuracy = correct / len(test_loader.dataset)\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Test Accuracy: {accuracy:.2f}, Test Loss: {test_loss:.2f}\")\n",
        "\n",
        "    def evaluate_and_display_random_samples(self, test_loader, num_samples=10):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a test dataset and display random samples.\n",
        "\n",
        "        Args:\n",
        "            test_loader: DataLoader for test data.\n",
        "            num_samples: Number of random samples to display.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        all_inputs = []\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        # Collect all data from the test loader\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                outputs = self.model(X_batch)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_inputs.append(X_batch)\n",
        "                all_labels.append(y_batch)\n",
        "                all_predictions.append(preds)\n",
        "\n",
        "        # Concatenate all batches\n",
        "        all_inputs = torch.cat(all_inputs)\n",
        "        all_labels = torch.cat(all_labels)\n",
        "        all_predictions = torch.cat(all_predictions)\n",
        "\n",
        "        # Select random indices\n",
        "        random_indices = np.random.choice(len(all_labels), num_samples, replace=False)\n",
        "\n",
        "        print(\"Random Samples: Actual vs Predicted\")\n",
        "        for i, idx in enumerate(random_indices):\n",
        "            actual_label = all_labels[idx].item()\n",
        "            predicted_label = all_predictions[idx].item()\n",
        "            print(f\"Sample {i + 1}: Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "\n",
        "def create_train_evaluate(hidden_layer, optimizer, batch_norm=False, dropout_prob=0.0, weight_decay=0.0):\n",
        "    \"\"\"\n",
        "    Create, train, and evaluate a model with specified parameters.\n",
        "\n",
        "    Args:\n",
        "        hidden_layer (int): Number of hidden layers.\n",
        "        optimizer (str): Optimizer to use.\n",
        "        batch_norm (bool): Whether to use batch normalization.\n",
        "        dropout_prob (float, optional): Dropout probability.\n",
        "        weight_decay (float, optional): Weight decay (L2 regularization).\n",
        "    \"\"\"\n",
        "    model_instance = Model(\n",
        "        hidden_layer=hidden_layer,\n",
        "        optimizer=optimizer,\n",
        "        input_size=input_size,\n",
        "        num_classes=num_classes,\n",
        "        use_batch_norm=batch_norm,\n",
        "        dropout_prob=dropout_prob,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "\n",
        "    model_instance.train_model(train_loader, val_loader, epochs=200)\n",
        "    model_instance.evaluate_model(test_loader)\n",
        "\n",
        "    return model_instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "gH10dobDIo86",
        "outputId": "240355b9-7fbf-4eb6-8ffc-190713b917df"
      },
      "outputs": [],
      "source": [
        "configs = [\n",
        "    {\"hidden_layer\": 1, \"optimizer\": \"SGD\"},\n",
        "    {\"hidden_layer\": 2, \"optimizer\": \"SGD\"},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    model_instance = create_train_evaluate(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configs = [\n",
        "    {\"hidden_layer\": 1, \"optimizer\": \"SGD\", \"batch_norm\": True, \"dropout_prob\": 0.15},\n",
        "    {\"hidden_layer\": 2, \"optimizer\": \"SGD\", \"batch_norm\": True, \"dropout_prob\": 0.3},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    model_instance = create_train_evaluate(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a-ORuGPik6cD",
        "outputId": "9797c87e-7088-4f5f-92fd-b0c8ea579a0b"
      },
      "outputs": [],
      "source": [
        "configs = [\n",
        "    {\"hidden_layer\": 1, \"optimizer\": \"SGD\", \"batch_norm\": True, \"dropout_prob\": 0.15, \"weight_decay\": 1e-4},\n",
        "    {\"hidden_layer\": 2, \"optimizer\": \"SGD\", \"batch_norm\": True, \"dropout_prob\": 0.3, \"weight_decay\": 1e-4},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    model_instance = create_train_evaluate(**config)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        # {\"hidden_layer\": 1, \"optimizer\": \"Adadelta\", \"batch_norm\": True, \"dropout_prob\": 0.55, \"weight_decay\": 1e-4},\n",
        "        # {\"hidden_layer\": 2, \"optimizer\": \"Adadelta\", \"batch_norm\": True, \"dropout_prob\": 0.7, \"weight_decay\": 1e-4},"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configs = [\n",
        "    {\"hidden_layer\": 1, \"optimizer\": \"Adam\", \"batch_norm\": True, \"dropout_prob\": 0.55, \"weight_decay\": 1e-4},\n",
        "    {\"hidden_layer\": 2, \"optimizer\": \"Adam\", \"batch_norm\": True, \"dropout_prob\": 0.7, \"weight_decay\": 1e-4},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    model_instance = create_train_evaluate(**config)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configs = [\n",
        "    {\"hidden_layer\": 1, \"optimizer\": \"Adadelta\", \"batch_norm\": True, \"dropout_prob\": 0.55, \"weight_decay\": 1e-4},\n",
        "    {\"hidden_layer\": 2, \"optimizer\": \"Adadelta\", \"batch_norm\": True, \"dropout_prob\": 0.7, \"weight_decay\": 1e-4},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    model_instance = create_train_evaluate(**config)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GiiZGBFTnse_",
        "outputId": "41f0103b-9d10-4824-cb0c-afdd35ae1c39"
      },
      "outputs": [],
      "source": [
        "configs = [\n",
        "    {\"hidden_layer\": 1, \"optimizer\": \"SGD\", \"batch_norm\": True, \"dropout_prob\": 0.15},\n",
        "    {\"hidden_layer\": 2, \"optimizer\": \"SGD\", \"batch_norm\": True, \"dropout_prob\": 0.3},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    model_instance = create_train_evaluate(**config)\n",
        "    model_instance.evaluate_and_display_random_samples(test_loader, num_samples=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
